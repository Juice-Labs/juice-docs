"use strict";(self.webpackChunkjuice_docs=self.webpackChunkjuice_docs||[]).push([[5132],{350:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>s,contentTitle:()=>o,default:()=>u,frontMatter:()=>r,metadata:()=>i,toc:()=>h});const i=JSON.parse('{"id":"juice/user-guide/cli-app/cache","title":"Cache","description":"Data sent from the client to the server (e.g. from the CPU to the remote GPU via cuMemcpyHtoD() or cuMemcpyHtoDAsync()), is cached on the server to save bandwidth if that data is referenced again. Caching improves load time on inference workloads by caching the model data between runs. Once the model is loaded and inference is being run, caching has no further effect on performance. Up to 16GB of data is cached per concurrent connection and that cached data will be reused for subsequent workloads run against the same server.","source":"@site/docs/juice/user-guide/cli-app/cache.md","sourceDirName":"juice/user-guide/cli-app","slug":"/juice/user-guide/cli-app/cache","permalink":"/docs/juice/user-guide/cli-app/cache","draft":false,"unlisted":false,"editUrl":"https://github.com/juice-labs/juice-docs/edit/master/docs/juice/user-guide/cli-app/cache.md","tags":[],"version":"current","sidebarPosition":10,"frontMatter":{"title":"Cache","sidebar_position":10},"sidebar":"tutorialSidebar","previous":{"title":"M2M Tokens","permalink":"/docs/juice/user-guide/cli-app/m2m-tokens"},"next":{"title":"CUDA Compatibility","permalink":"/docs/juice/user-guide/cli-app/cuda-compatibility"}}');var c=n(4848),a=n(8453);const r={title:"Cache",sidebar_position:10},o="Cache",s={},h=[{value:"On Windows, the cache is stored in the numbered directories beneath:",id:"on-windows-the-cache-is-stored-in-the-numbered-directories-beneath",level:4},{value:"On Linux, the cache can be found here:",id:"on-linux-the-cache-can-be-found-here",level:4},{value:"Example:",id:"example",level:3}];function d(e){const t={admonition:"admonition",code:"code",h1:"h1",h3:"h3",h4:"h4",header:"header",p:"p",pre:"pre",strong:"strong",...(0,a.R)(),...e.components};return(0,c.jsxs)(c.Fragment,{children:[(0,c.jsx)(t.header,{children:(0,c.jsx)(t.h1,{id:"cache",children:"Cache"})}),"\n",(0,c.jsxs)(t.p,{children:["Data sent from the client to the server (e.g. from the CPU to the remote GPU via ",(0,c.jsx)(t.strong,{children:"cuMemcpyHtoD()"})," or ",(0,c.jsx)(t.strong,{children:"cuMemcpyHtoDAsync()"}),"), is cached on the server to save bandwidth if that data is referenced again. Caching improves load time on inference workloads by caching the model data between runs. Once the model is loaded and inference is being run, caching has no further effect on performance. Up to 16GB of data is cached per concurrent connection and that cached data will be reused for subsequent workloads run against the same server."]}),"\n",(0,c.jsx)(t.p,{children:"Caching improves performance during a training workload by caching the training data. You'll want to ensure that the size of your training data fits within the fixed 16GB cache size or structure training so that the working set of training data fits within that 16GB. If you can't limit your working set this way, then you might get better performance by disabling the cache from the Juice application or with the juice run option --disable-cache."}),"\n",(0,c.jsx)(t.p,{children:"Cache eviction occurs in FIFO order; the oldest entry is evicted to make room for any new entry once the 16GB size limit is reached. Make sure that your working set (i.e. the model or training data) fits within this space for ideal performance."}),"\n",(0,c.jsx)(t.p,{children:"The cached data and amount of GPU memory are independent. Plenty of results and values generated on the GPU will be stored in memory without needing to be cached.  The cache size should be related to the size of the working set of data for the workload rather than the amount of GPU memory available on the server."}),"\n",(0,c.jsx)(t.h4,{id:"on-windows-the-cache-is-stored-in-the-numbered-directories-beneath",children:"On Windows, the cache is stored in the numbered directories beneath:"}),"\n",(0,c.jsx)(t.pre,{children:(0,c.jsx)(t.code,{className:"language-powershell",children:"%LocalAppData%\\Juice GPU\\cache and C:\\Windows\\ServiceProfiles\\juice\\AppData\\Local\\Juice GPU\\cache\n"})}),"\n",(0,c.jsx)(t.h4,{id:"on-linux-the-cache-can-be-found-here",children:"On Linux, the cache can be found here:"}),"\n",(0,c.jsx)(t.pre,{children:(0,c.jsx)(t.code,{className:"language-powershell",children:"~/.cache/Juice/cache\n"})}),"\n",(0,c.jsx)(t.admonition,{type:"note",children:(0,c.jsx)(t.p,{children:"You may delete these numbered directories while not sharing your GPU to recover disk space."})}),"\n",(0,c.jsx)(t.p,{children:"Concurrent means that if this agent is being used by more than one connection (i.e. different users, different workloads, etc.) at the same time, each concurrent connection might also consume up to 16GB of cache storage in the agent.  There is currently no explicit limit on the number of concurrent connections a server can accept beyond the GPUs work limit."}),"\n",(0,c.jsx)(t.h3,{id:"example",children:"Example:"}),"\n",(0,c.jsx)(t.p,{children:"100 concurrent connections can drive the cache size to up to 100 x 16GB = 1600 GBs."})]})}function u(e={}){const{wrapper:t}={...(0,a.R)(),...e.components};return t?(0,c.jsx)(t,{...e,children:(0,c.jsx)(d,{...e})}):d(e)}},8453:(e,t,n)=>{n.d(t,{R:()=>r,x:()=>o});var i=n(6540);const c={},a=i.createContext(c);function r(e){const t=i.useContext(a);return i.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function o(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(c):e.components||c:r(e.components),i.createElement(a.Provider,{value:t},e.children)}}}]);