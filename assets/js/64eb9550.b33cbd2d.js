"use strict";(self.webpackChunkjuice_docs=self.webpackChunkjuice_docs||[]).push([[601],{4483:(e,r,n)=>{n.r(r),n.d(r,{assets:()=>c,contentTitle:()=>l,default:()=>h,frontMatter:()=>a,metadata:()=>t,toc:()=>d});const t=JSON.parse('{"id":"juice/pro-users/performance-latency/example-workloads","title":"Example Workloads","description":"In addition to hand-rolled CUDA code, juice supports third party frameworks like PyTorch and TensorFlow. Using external code also comes with performance implications. In general, for deep learning frameworks","source":"@site/docs/juice/pro-users/performance-latency/example-workloads.md","sourceDirName":"juice/pro-users/performance-latency","slug":"/juice/pro-users/performance-latency/example-workloads","permalink":"/juice-docs/docs/juice/pro-users/performance-latency/example-workloads","draft":false,"unlisted":false,"editUrl":"https://github.com/juice-labs/juice-docs/edit/master/docs/juice/pro-users/performance-latency/example-workloads.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"title":"Example Workloads","sidebar_position":3},"sidebar":"tutorialSidebar","previous":{"title":"Example","permalink":"/juice-docs/docs/juice/pro-users/performance-latency/example"},"next":{"title":"Inviting Users","permalink":"/juice-docs/docs/juice/admin/inviting-users"}}');var s=n(4848),i=n(8453);const a={title:"Example Workloads",sidebar_position:3},l="Example Workloads",c={},d=[];function o(e){const r={code:"code",h1:"h1",header:"header",img:"img",li:"li",p:"p",pre:"pre",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,i.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(r.header,{children:(0,s.jsx)(r.h1,{id:"example-workloads",children:"Example Workloads"})}),"\n",(0,s.jsx)(r.p,{children:"In addition to hand-rolled CUDA code, juice supports third party frameworks like PyTorch and TensorFlow. Using external code also comes with performance implications. In general, for deep learning frameworks: the more useful work the gpu does before having to report back to the host the more efficiently juice will perform. We can see this right away by comparing the single epoch timings for training some simple neural networks and one moderate complexity network:"}),"\n",(0,s.jsx)(r.p,{children:(0,s.jsx)(r.img,{alt:"Example Workload",src:n(3010).A+"",width:"1006",height:"791"})}),"\n",(0,s.jsx)(r.p,{children:"The MNIST-X workloads are composed of X simple dense layers applied in sequence to a small-sized dataset of images. Dense layers are often implemented as a single matrix multiplication performed on the GPU which amounts to very little actual work being performed. This results in the latency overhead being poorly hidden. The Image Segmentation (Im. Seg.) workflow runs a network that utilizes much more complicated layers and can therefore hide the overhead much better. To produce the above figure each network was trained for two epochs and the second epoch timing was reported to avoid any \u201cwarm-up\u201d artifacts. This was repeated five times to avoid any network related hiccups."}),"\n",(0,s.jsx)(r.p,{children:(0,s.jsx)(r.img,{alt:"Example Workload 2",src:n(8826).A+"",width:"637",height:"456"})}),"\n",(0,s.jsx)(r.p,{children:"Further examples of the impact of latency and bandwidth on larger networks:"}),"\n",(0,s.jsxs)(r.ul,{children:["\n",(0,s.jsxs)(r.li,{children:["\n",(0,s.jsx)(r.p,{children:"Client - AMD EPYC 7413, 8 CPUs, 32GB ram"}),"\n"]}),"\n",(0,s.jsxs)(r.li,{children:["\n",(0,s.jsx)(r.p,{children:"Server - AMD EPYC 7413, 8 CPUs, 32GB ram, NVIDIA A40 48GB"}),"\n"]}),"\n",(0,s.jsxs)(r.li,{children:["\n",(0,s.jsx)(r.p,{children:"Latency between Client <-> Server, 0.6ms"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(r.table,{children:[(0,s.jsx)(r.thead,{children:(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.th,{style:{textAlign:"center"},children:"Model Name"}),(0,s.jsx)(r.th,{style:{textAlign:"center"},children:"Mode"}),(0,s.jsx)(r.th,{style:{textAlign:"center"},children:"Average Time"}),(0,s.jsx)(r.th,{style:{textAlign:"center"},children:"juice as % of Native"})]})}),(0,s.jsxs)(r.tbody,{children:[(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{style:{textAlign:"center"},children:"EleutherAI/pythia-70m-deduped"}),(0,s.jsx)(r.td,{style:{textAlign:"center"},children:"Native"}),(0,s.jsx)(r.td,{style:{textAlign:"center"},children:"6.471998405"}),(0,s.jsx)(r.td,{style:{textAlign:"center"}})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{style:{textAlign:"center"},children:"EleutherAI/pythia-70m-deduped"}),(0,s.jsx)(r.td,{style:{textAlign:"center"},children:"juice"}),(0,s.jsx)(r.td,{style:{textAlign:"center"},children:"6.703869677"}),(0,s.jsx)(r.td,{style:{textAlign:"center"},children:"96%"})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{style:{textAlign:"center"},children:"EleutherAI/pythia-6.9b-deduped"}),(0,s.jsx)(r.td,{style:{textAlign:"center"},children:"Native"}),(0,s.jsx)(r.td,{style:{textAlign:"center"},children:"41.1083591"}),(0,s.jsx)(r.td,{style:{textAlign:"center"}})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{style:{textAlign:"center"},children:"EleutherAI/pythia-6.9b-deduped"}),(0,s.jsx)(r.td,{style:{textAlign:"center"},children:"juice"}),(0,s.jsx)(r.td,{style:{textAlign:"center"},children:"37.7191237"}),(0,s.jsx)(r.td,{style:{textAlign:"center"},children:"109%"})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{style:{textAlign:"center"},children:"EleutherAI/pythia-12b-deduped"}),(0,s.jsx)(r.td,{style:{textAlign:"center"},children:"Native"}),(0,s.jsx)(r.td,{style:{textAlign:"center"},children:"60.10230432"}),(0,s.jsx)(r.td,{style:{textAlign:"center"}})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{style:{textAlign:"center"},children:"EleutherAI/pythia-12b-deduped"}),(0,s.jsx)(r.td,{style:{textAlign:"center"},children:"juice"}),(0,s.jsx)(r.td,{style:{textAlign:"center"},children:"54.84337864"}),(0,s.jsx)(r.td,{style:{textAlign:"center"},children:"109%"})]})]})]}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{children:'# listing 1\r\n\r\n#include <array> \r\n\r\n#include <chrono> \r\n\r\n#include <cmath> \r\n\r\n#include <cstdlib> \r\n\r\n#include <cuda_runtime_api.h> \r\n\r\n#include <driver_types.h> \r\n\r\n#include <iostream> \r\n\r\n \r\n\r\n#define N 50000 \r\n\r\n#define NSTEP 1000 \r\n\r\n#define NKERNEL 1000 \r\n\r\n \r\n\r\n#define TIC auto start = std::chrono::high_resolution_clock::now(); \r\n\r\n#define TOC(msg) \\ \r\n\r\n  auto end = std::chrono::high_resolution_clock::now();\\ \r\n\r\n  std::chrono::duration<double> duration = end - start;\\ \r\n\r\n  std::cout << msg << " elapsed: " << duration.count() << "s" << std::endl; \r\n\r\n \r\n\r\n__global__ void shortKernel(float * dst, float * src){ \r\n\r\n  int idx=blockIdx.x*blockDim.x+threadIdx.x; \r\n\r\n  if(idx<N) dst[idx]=1.23*src[idx] + 4.0; \r\n\r\n} \r\n\r\n \r\n\r\nint main(int argc, char* argv[]) { \r\n\r\n  cudaStream_t stream{}; \r\n\r\n  cudaStreamCreate(&stream); \r\n\r\n \r\n\r\n  float *devSrc, *devDst; \r\n\r\n  cudaMalloc(&devSrc, N * sizeof(float)); \r\n\r\n  cudaMalloc(&devDst, N * sizeof(float)); \r\n\r\n \r\n\r\n  int threads = 512; \r\n\r\n  int blocks = std::ceilf((float)N / threads); \r\n\r\n  { \r\n\r\n    TIC \r\n\r\n    for(int istep=0; istep<NSTEP; istep++){ \r\n\r\n      for(int ikrnl=0; ikrnl<NKERNEL; ikrnl++){ \r\n\r\n        shortKernel<<<blocks, threads, 0, stream>>>(devDst, devSrc); \r\n\r\n        cudaStreamSynchronize(stream); \r\n\r\n      } \r\n\r\n    } \r\n\r\n    TOC("Naive") \r\n\r\n  } \r\n\r\n  { \r\n\r\n    TIC \r\n\r\n    for(int istep=0; istep<NSTEP; istep++){ \r\n\r\n      for(int ikrnl=0; ikrnl<NKERNEL; ikrnl++){ \r\n\r\n        shortKernel<<<blocks, threads, 0, stream>>>(devDst, devSrc); \r\n\r\n      } \r\n\r\n      cudaStreamSynchronize(stream); \r\n\r\n    } \r\n\r\n    TOC("Interleaved") \r\n\r\n  } \r\n\r\n  { \r\n\r\n    TIC \r\n\r\n    bool graphCreated=false; \r\n\r\n    cudaGraph_t graph{}; \r\n\r\n    cudaGraphExec_t instance{}; \r\n\r\n    for(int istep=0; istep<NSTEP; istep++){ \r\n\r\n      if(!graphCreated){ \r\n\r\n        cudaStreamBeginCapture(stream, cudaStreamCaptureModeGlobal); \r\n\r\n        for(int ikrnl=0; ikrnl<NKERNEL; ikrnl++){ \r\n\r\n          shortKernel<<<blocks, threads, 0, stream>>>(devDst, devSrc); \r\n\r\n        } \r\n\r\n        cudaStreamEndCapture(stream, &graph); \r\n\r\n        cudaGraphInstantiate(&instance, graph, NULL, NULL, 0); \r\n\r\n        graphCreated=true; \r\n\r\n      } \r\n\r\n      cudaGraphLaunch(instance, stream); \r\n\r\n      cudaStreamSynchronize(stream); \r\n\r\n    } \r\n\r\n    TOC("Graph") \r\n\r\n  } \r\n\r\n  cudaDeviceReset(); \r\n\r\n  return 0; \r\n\r\n} \n'})}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{children:'# listing 2\r\n\r\n \r\n\r\nconst int batchesInFlight = 8; \r\n\r\nstd::array<float *, batchesInFlight> devSrcs{}; \r\n\r\nstd::array<float *, batchesInFlight> devDsts{}; \r\n\r\nstd::array<cudaEvent_t, batchesInFlight> events{}; \r\n\r\nstd::array<int, batchesInFlight> stepsTaken{}; \r\n\r\n \r\n\r\nfor (int i = 0; i < batchesInFlight; ++i) { \r\n\r\n  cudaMalloc(&devSrcs[i], N * sizeof(float)); \r\n\r\n  cudaMalloc(&devDsts[i], N * sizeof(float)); \r\n\r\n  cudaEventCreate(&events[i]); \r\n\r\n  stepsTaken[i] = 0; \r\n\r\n} \r\n\r\n \r\n\r\n{ \r\n\r\n  TIC; \r\n\r\n  bool graphCreated=false; \r\n\r\n  bool batchesProcessing=true; \r\n\r\n  std::array<cudaGraph_t, batchesInFlight> graphs{}; \r\n\r\n  std::array<cudaGraphExec_t, batchesInFlight> instances{}; \r\n\r\n \r\n\r\n  for (int i = 0; i < batchesInFlight; ++i) { \r\n\r\n    cudaStreamBeginCapture(stream, cudaStreamCaptureModeGlobal); \r\n\r\n    for(int ikrnl=0; ikrnl<NKERNEL; ikrnl++){ \r\n\r\n      shortKernel<<<blocks, threads, 0, stream>>>(devDsts[i], devSrcs[i]); \r\n\r\n    } \r\n\r\n    cudaStreamEndCapture(stream, &graphs[i]); \r\n\r\n    cudaGraphInstantiate(&instances[i], graphs[i], NULL, NULL, 0); \r\n\r\n  } \r\n\r\n  while (batchesProcessing) { \r\n\r\n    batchesProcessing = false; \r\n\r\n    for (int i = 0; i < batchesInFlight; ++i) { \r\n\r\n      if (stepsTaken[i] < NSTEP) { \r\n\r\n        batchesProcessing = true; \r\n\r\n        if (cudaEventQuery(events[i]) == cudaSuccess) { \r\n\r\n          stepsTaken[i]++; \r\n\r\n          cudaGraphLaunch(instances[i], stream); \r\n\r\n          cudaEventRecord(events[i], stream); \r\n\r\n        } \r\n\r\n      } \r\n\r\n    } \r\n\r\n  } \r\n\r\n  cudaStreamSynchronize(stream); \r\n\r\n  TOC("Pipes"); \r\n} \n'})})]})}function h(e={}){const{wrapper:r}={...(0,i.R)(),...e.components};return r?(0,s.jsx)(r,{...e,children:(0,s.jsx)(o,{...e})}):o(e)}},3010:(e,r,n)=>{n.d(r,{A:()=>t});const t=n.p+"assets/images/example_workload-05c528af749dcebcdc5771eba35dca5c.png"},8826:(e,r,n)=>{n.d(r,{A:()=>t});const t=n.p+"assets/images/example_workload2-858f7310911b70e3f8329bbf85be5d98.png"},8453:(e,r,n)=>{n.d(r,{R:()=>a,x:()=>l});var t=n(6540);const s={},i=t.createContext(s);function a(e){const r=t.useContext(i);return t.useMemo((function(){return"function"==typeof e?e(r):{...r,...e}}),[r,e])}function l(e){let r;return r=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),t.createElement(i.Provider,{value:r},e.children)}}}]);